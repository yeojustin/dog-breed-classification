{"cells":[{"cell_type":"code","source":["import os\n","gdrive_path = \"/content/drive/MyDrive/Murdoch/ICT303/A2\"\n","\n","if os.path.exists(gdrive_path):\n","    print(\"Drive is already mounted.\")\n","else:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"metadata":{"id":"NbH3PGIwkMki","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712035807726,"user_tz":-480,"elapsed":107087,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"013a4b9a-0d3b-4733-ec80-0d493457cec3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### **Downloading the Data Set**\n"],"metadata":{"id":"he01H0Em-AlP"}},{"cell_type":"code","source":["import zipfile\n","from tqdm import tqdm\n","\n","data_dir = \"/content/drive/MyDrive/Murdoch/ICT303/A2/kaggle_dog\"\n","\n","zipfiles = ['train.zip', 'test.zip', 'labels.csv.zip']\n","for f in tqdm(zipfiles):\n","  with zipfile.ZipFile(data_dir + '/' + f, 'r') as z:\n","    z.extractall(data_dir)"],"metadata":{"id":"nQl2ToXAuuIF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711031643304,"user_tz":-480,"elapsed":255356,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"0c6f1fec-c91d-41a9-ca8d-275e39f1b4b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [04:15<00:00, 85.01s/it]\n"]}]},{"cell_type":"markdown","source":["### **1.2. Organizing the Data Set**\n","\n","Next, we define the reorg_train_valid function to split the validation set from the original Kaggle competition training set. The parameter valid_ratio in this function is the ratio of the number of examples of each dog breeds in the validation set to the number of examples of the\n","breed with the least examples (66) in the original training set.\n","\n","After organizing the data, images of the same breed will be placed in the same folder so that we can read them later."],"metadata":{"id":"B6lWE7t9-Cfn"}},{"cell_type":"code","source":["# Let's first install d2l package, since we will need some functions from this package\n","! pip install d2l==1.0.0a1.post0"],"metadata":{"id":"cl3UvrUBw70C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Function to create directory if it doesnt exist, instead of using the d2l module\n","import os\n","def mkdir_if_not_exist(path):\n","    if not os.path.exists(os.path.join(*path)):\n","        os.makedirs(os.path.join(*path))"],"metadata":{"id":"jJlEXBJkmbUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","import d2l\n","import shutil\n","import os\n","import math\n","\n","def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n","  # The number of examples of the least represented breed in the training set.\n","  min_n_train_per_label = (\n","      collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n","\n","  # The number of examples of each breed in the validation set.\n","  n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n","  label_count = {}\n","  for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n","    idx = train_file.split('.')[0]\n","    label = idx_label[idx]\n","\n","    mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n","\n","    shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                os.path.join(data_dir, input_dir, 'train_valid', label))\n","\n","    if label not in label_count or label_count[label] < n_valid_per_label:\n","      mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n","      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                  os.path.join(data_dir, input_dir, 'valid', label))\n","      label_count[label] = label_count.get(label, 0) + 1\n","\n","    else:\n","      mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n","      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n","                  os.path.join(data_dir, input_dir, 'train', label))"],"metadata":{"id":"B5YtyOyyvrS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Obtaining and Organizing the Data Set**\n","\n","The competition data is divided into a training set and testing set:\n","- The training set contains $10,222$ color images.\n","- The testing set contains 10,357 color images.\n","\n","The images in both sets are in JPEG format. Each image contains three channels (R, G and B). The images have  different heights and widths.\n","\n","There are $120$ breeds of dogs in the training set, e.g., *Labradors, Poodles, Dachshunds,\n","Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers*."],"metadata":{"id":"QYa8reVO9OBQ"}},{"cell_type":"markdown","source":["The `reorg_dog_data` function below is used to read the training data labels, segment the validation set, and organize the training set."],"metadata":{"id":"hO1gPYXCyX_t"}},{"cell_type":"code","source":["def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n","  # Read the training data labels.\n","  with open(os.path.join(data_dir, label_file), 'r') as f:\n","    # Skip the file header line (column name).\n","    lines = f.readlines()[1:]\n","    tokens = [l.rstrip().split(',') for l in lines]\n","    idx_label = dict(((idx, label) for idx, label in tokens))\n","\n","  reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n","\n","  # Organize the training set.\n","  mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n","  for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n","    shutil.copy(os.path.join(data_dir, test_dir, test_file),\n","                os.path.join(data_dir, input_dir, 'test', 'unknown'))"],"metadata":{"id":"gdsp7bSryXbQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["During actual training and testing, we would use the entire Kaggle Competition data set and call the reorg_dog_data function to organize the data set. Likewise, we would need to set the batch_size to a larger integer, such as 128."],"metadata":{"id":"trqywyYRysYi"}},{"cell_type":"code","source":["label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n","input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n","reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"],"metadata":{"id":"A3o_SZtRy4qt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DONE"],"metadata":{"id":"opIuO-_ctP9J"}},{"cell_type":"markdown","source":["## **Image Augmentation**\n","\n","Sometimes, when we do not have enough images to train our deep learning model, we data augmentation to simulate new data. For example, in the case of images, assume we only have $10$ images per class. We can create more instance by applying transformations to these images. For example, if the image is of a standin dog, we can rotate it $90$ and $180$ degrees to create two additional instances of the same dog. We can also scale it, etc.\n","\n","Here are some more image augmentation operations that might be useful.\n","\n","Start by training your model on the data set, the way it is provided. Then, think of the types of transformations you can apply to the training images to improve the performance.\n","\n","You can find more about how to apply transformations to images in this [link](https://pytorch.org/vision/stable/transforms.html)."],"metadata":{"id":"Mh_N5wsV-FUv"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import plotly.express as px\n","\n","labels_df = pd.read_csv(f\"{gdrive_path}/kaggle_dog/labels.csv\")"],"metadata":{"id":"gUndJsh5U_RX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_df['breed'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60Fx1IG2WEgT","executionInfo":{"status":"ok","timestamp":1711384577530,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"836756bd-759d-478a-f446-a1e864c82300"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["scottish_deerhound      126\n","maltese_dog             117\n","afghan_hound            116\n","entlebucher             115\n","bernese_mountain_dog    114\n","                       ... \n","golden_retriever         67\n","brabancon_griffon        67\n","komondor                 67\n","eskimo_dog               66\n","briard                   66\n","Name: breed, Length: 120, dtype: int64"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["labels_df['id'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOxkm59SkPnO","executionInfo":{"status":"ok","timestamp":1711384577530,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"d9a9bf46-1481-41c3-89b4-f4b714bfc980"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['000bec180eb18c7604dcecc8fe0dba07',\n","       '001513dfcb2ffafc82cccf4d8bbaba97',\n","       '001cdf01b096e06d78e9e5112d419397', ...,\n","       'ffe2ca6c940cddfee68fa3cc6c63213f',\n","       'ffe5f6d8e2bff356e9482a80a6e29aac',\n","       'fff43b07992508bc822f33d8ffd902ae'], dtype=object)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["px.bar(labels_df['breed'].value_counts(), title='Count of Dog Breeds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"3ufxmdtgVFzg","executionInfo":{"status":"ok","timestamp":1711384579547,"user_tz":-480,"elapsed":2021,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"d7ddf745-0ef2-4dd1-b765-837444926cb8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"28a23be3-8332-46bc-a3bd-11848a3bdcf9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"28a23be3-8332-46bc-a3bd-11848a3bdcf9\")) {                    Plotly.newPlot(                        \"28a23be3-8332-46bc-a3bd-11848a3bdcf9\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=breed\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"breed\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"breed\",\"offsetgroup\":\"breed\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"scottish_deerhound\",\"maltese_dog\",\"afghan_hound\",\"entlebucher\",\"bernese_mountain_dog\",\"shih-tzu\",\"great_pyrenees\",\"pomeranian\",\"basenji\",\"samoyed\",\"airedale\",\"tibetan_terrier\",\"leonberg\",\"cairn\",\"beagle\",\"japanese_spaniel\",\"australian_terrier\",\"blenheim_spaniel\",\"miniature_pinscher\",\"irish_wolfhound\",\"lakeland_terrier\",\"saluki\",\"papillon\",\"norwegian_elkhound\",\"whippet\",\"siberian_husky\",\"pug\",\"chow\",\"italian_greyhound\",\"pembroke\",\"ibizan_hound\",\"border_terrier\",\"newfoundland\",\"lhasa\",\"silky_terrier\",\"dandie_dinmont\",\"bedlington_terrier\",\"sealyham_terrier\",\"irish_setter\",\"rhodesian_ridgeback\",\"old_english_sheepdog\",\"collie\",\"boston_bull\",\"schipperke\",\"bouvier_des_flandres\",\"kelpie\",\"english_foxhound\",\"african_hunting_dog\",\"bloodhound\",\"bluetick\",\"weimaraner\",\"saint_bernard\",\"labrador_retriever\",\"english_setter\",\"chesapeake_bay_retriever\",\"norfolk_terrier\",\"scotch_terrier\",\"groenendael\",\"wire-haired_fox_terrier\",\"basset\",\"kerry_blue_terrier\",\"irish_terrier\",\"yorkshire_terrier\",\"greater_swiss_mountain_dog\",\"malamute\",\"gordon_setter\",\"keeshond\",\"west_highland_white_terrier\",\"dingo\",\"toy_poodle\",\"mexican_hairless\",\"clumber\",\"affenpinscher\",\"standard_poodle\",\"miniature_poodle\",\"staffordshire_bullterrier\",\"toy_terrier\",\"welsh_springer_spaniel\",\"irish_water_spaniel\",\"appenzeller\",\"sussex_spaniel\",\"miniature_schnauzer\",\"norwich_terrier\",\"black-and-tan_coonhound\",\"dhole\",\"shetland_sheepdog\",\"rottweiler\",\"cardigan\",\"bull_mastiff\",\"boxer\",\"english_springer\",\"german_short-haired_pointer\",\"pekinese\",\"great_dane\",\"borzoi\",\"american_staffordshire_terrier\",\"doberman\",\"cocker_spaniel\",\"malinois\",\"brittany_spaniel\",\"standard_schnauzer\",\"curly-coated_retriever\",\"flat-coated_retriever\",\"border_collie\",\"redbone\",\"kuvasz\",\"chihuahua\",\"soft-coated_wheaten_terrier\",\"french_bulldog\",\"vizsla\",\"otterhound\",\"german_shepherd\",\"walker_hound\",\"tibetan_mastiff\",\"giant_schnauzer\",\"golden_retriever\",\"brabancon_griffon\",\"komondor\",\"eskimo_dog\",\"briard\"],\"xaxis\":\"x\",\"y\":[126,117,116,115,114,112,111,111,110,109,107,107,106,106,105,105,102,102,102,101,99,99,96,95,95,95,94,93,92,92,91,91,91,90,90,89,89,88,88,88,87,87,87,86,86,86,86,86,85,85,85,84,84,83,83,83,82,82,82,82,82,82,82,82,81,81,81,81,80,80,80,80,80,79,79,79,79,79,78,78,78,78,78,77,76,76,76,76,75,75,75,75,75,75,75,74,74,74,73,73,72,72,72,72,72,71,71,71,70,70,69,69,69,69,69,67,67,67,66,66],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Count of Dog Breeds\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('28a23be3-8332-46bc-a3bd-11848a3bdcf9');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## **Loading (Reading) the Data Set**"],"metadata":{"id":"QCUUbgO0-OcV"}},{"cell_type":"code","source":["!pip install skorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZjXcFEXkjvF","executionInfo":{"status":"ok","timestamp":1712036709756,"user_tz":-480,"elapsed":6379,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"b7fb02ae-1680-4421-892f-dbd780fb8f3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting skorch\n","  Downloading skorch-0.15.0-py3-none-any.whl (239 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/239.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.4.0)\n","Installing collected packages: skorch\n","Successfully installed skorch-0.15.0\n"]}]},{"cell_type":"code","source":["#@title All imports\n","import os\n","from sklearn.model_selection import GridSearchCV\n","from skorch import NeuralNetClassifier\n","from torch import nn\n","from torch import optim\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, models, datasets\n","from torchsummary import summary\n","import numpy as np\n","import torch\n","from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n","from skorch.callbacks import LoadInitState\n","from skorch.callbacks import EpochScoring\n","from skorch.helper import SliceDataset\n","from PIL import ImageFile, Image\n","from skorch.callbacks import Callback\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as F"],"metadata":{"id":"tgPD_oOWkhzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Custom transformations dictionary for all datasets for indexing use\n","transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=(256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ]),\n","    'train_valid': transforms.Compose([\n","        transforms.Resize(size=(256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=(256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","}\n"],"metadata":{"id":"18oF38wglT_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similar to previous labs, write here the Python code tat reads the training, validation and test set."],"metadata":{"id":"HKE7gFVE1Heu"}},{"cell_type":"code","source":["## Create the datasets when the images are arranged in the specific format: './class_name/xx.png'\n","dir = f\"{gdrive_path}/kaggle_dog/train_valid_test\"\n","ds = {x: datasets.ImageFolder(os.path.join(dir, x), transforms[x])\n","         for x in ['train', 'valid', 'test', 'train_valid']}\n","\n","## Wrap the image tensors and labels separately as SliceDataset objects so that they can be used in a grid search.\n","## X as input  and y as output\n","\n","data = ['train', 'valid', 'test', 'train_valid']\n","data_dict = {}\n","\n","for x in data:\n","    data_dict[x] = {'X': SliceDataset(ds[x], idx=0), 'y': SliceDataset(ds[x], idx=1)}"],"metadata":{"id":"hYj06w2FlXg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Setting default device, GPU first or else CPU\n","def get_default_device():\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","device = get_default_device()\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oz6h1UF-oT6S","executionInfo":{"status":"ok","timestamp":1712060087657,"user_tz":-480,"elapsed":1035,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"aefe39e9-33a9-4ee6-d9d0-4378aa19e979"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["## Define ResNet class\n","class CustomResNet(nn.Module):\n","    def __init__(self, pretrained_model):\n","        super().__init__()\n","        self.pretrained = pretrained_model\n","        # Add custom layers here.\n","        self.append1 = nn.Linear(250, 120)\n","\n","    def forward(self, X):\n","        # Compute the output given the input X\n","        X = self.pretrained(X)\n","        X = self.append1(X)\n","        return X"],"metadata":{"id":"vkxOoH1lofGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Define the pretrained ResNet model\n","resnet_pretrained = models.resnet18(pretrained=True)\n","num_ftrs = resnet_pretrained.fc.in_features\n","for param in resnet_pretrained.parameters():\n","    param.requires_grad = False\n","\n","resnet_pretrained.fc = nn.Linear(num_ftrs, 250)\n","\n","## Instantiate the custom model\n","custom_model = CustomResNet(pretrained_model=resnet_pretrained)\n","\n","## Move the model to the appropriate device (run on Cuda GPU)\n","custom_model = custom_model.to(device)"],"metadata":{"id":"5DOPRnTtprj9","executionInfo":{"status":"ok","timestamp":1712037266782,"user_tz":-480,"elapsed":869,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"826c01f8-cd0f-4b02-c955-908f145c777b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 152MB/s]\n"]}]},{"cell_type":"code","source":["## Creating a checkpoint when validation loss improves.\n","cp = Checkpoint(dirname=f'{gdrive_path}/experiment12', f_params='params_{last_epoch[epoch]}.pt', monitor='valid_loss_best')\n","\n","# Defining a neural net classifier based on the model defined earlier.\n","net = NeuralNetClassifier(\n","    custom_model,\n","    criterion=nn.CrossEntropyLoss(),\n","    lr=0.1,\n","    optimizer=torch.optim.Adam,\n","    iterator_train__shuffle=True,\n","    device=device,\n","    callbacks=[cp, EpochScoring(scoring='accuracy', name='train_acc', on_train=True)]\n",")\n","\n","## Grid of hyperparameters to run grid search over.\n","params = {\n","    'lr': [0.1, 0.01],\n","    # 'max_epochs': [5, 10],\n","    'batch_size': [128]\n","}\n","\n","## Instantiating GridsearchCV for hyperparameter tuning. Default number of folds is 5.\n","gs = GridSearchCV(net, params, refit=True, cv=2, scoring='accuracy')\n","\n","## Fitting the model on the train_valid dataset using the best set of hyperparameters found.\n","search = gs.fit(data_dict['train_valid']['X'] , data_dict['train_valid']['y'])"],"metadata":{"id":"JrRCJlR4qxkN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbd7f742-f26c-40ec-f07f-23baf8e0da75","executionInfo":{"status":"ok","timestamp":1712047396015,"user_tz":-480,"elapsed":588450,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n","-------  -----------  ------------  -----------  ------------  ----  -------\n","      1       \u001b[36m0.0250\u001b[0m     \u001b[32m1105.9892\u001b[0m       \u001b[35m0.0469\u001b[0m      \u001b[31m822.4342\u001b[0m     +  82.7755\n","      2       0.2243      \u001b[32m398.0304\u001b[0m       \u001b[35m0.3284\u001b[0m      \u001b[31m185.2923\u001b[0m     +  83.5915\n","      3       0.4190      \u001b[32m160.0951\u001b[0m       \u001b[35m0.4301\u001b[0m      \u001b[31m161.2912\u001b[0m     +  82.6588\n","      4       0.6049       \u001b[32m79.3159\u001b[0m       \u001b[35m0.5582\u001b[0m       \u001b[31m91.8884\u001b[0m     +  82.3307\n","      5       0.6734       \u001b[32m51.8980\u001b[0m       0.5318      113.8394        82.1623\n","      6       0.6539       59.0889       0.5396      103.4143        82.1744\n","      7       0.6959       \u001b[32m46.6743\u001b[0m       0.5327      110.8362        81.3448\n","      8       0.7370       \u001b[32m37.8860\u001b[0m       0.5455      105.3266        81.8155\n","      9       0.7224       45.4747       \u001b[35m0.5699\u001b[0m      105.0908        81.3315\n","     10       0.7285       45.1786       0.5523      121.3597        82.7499\n","  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n","-------  -----------  ------------  -----------  ------------  ----  -------\n","      1       \u001b[36m0.0198\u001b[0m     \u001b[32m1247.8183\u001b[0m       \u001b[35m0.0459\u001b[0m      \u001b[31m936.3088\u001b[0m     +  80.9345\n","      2       0.2140      \u001b[32m403.8863\u001b[0m       \u001b[35m0.3998\u001b[0m      \u001b[31m158.4274\u001b[0m     +  80.3432\n","      3       0.4479      \u001b[32m137.5687\u001b[0m       \u001b[35m0.5024\u001b[0m      \u001b[31m102.5776\u001b[0m     +  81.6492\n","      4       0.5819       \u001b[32m74.0287\u001b[0m       \u001b[35m0.5738\u001b[0m       \u001b[31m78.0123\u001b[0m     +  81.2839\n","      5       0.5122      133.0685       0.4330      219.8674        80.3921\n","      6       0.6292       81.0259       0.5728       92.3261        80.2532\n","      7       0.7136       \u001b[32m49.2606\u001b[0m       \u001b[35m0.6139\u001b[0m       87.4772        80.7927\n","      8       0.7546       \u001b[32m35.7727\u001b[0m       0.5621      104.0764        80.4789\n","      9       0.7008       57.5098       0.5357      142.4656        80.0770\n","     10       0.7231       49.2488       0.5924      108.7663        81.5597\n","  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n","-------  -----------  ------------  -----------  ------------  ----  -------\n","      1       \u001b[36m0.1054\u001b[0m       \u001b[32m10.7528\u001b[0m       \u001b[35m0.3109\u001b[0m        \u001b[31m4.2365\u001b[0m     +  81.7020\n","      2       0.5120        \u001b[32m2.0075\u001b[0m       \u001b[35m0.5787\u001b[0m        \u001b[31m1.5564\u001b[0m     +  82.5291\n","      3       0.7214        \u001b[32m0.9258\u001b[0m       \u001b[35m0.6129\u001b[0m        \u001b[31m1.3029\u001b[0m     +  81.2014\n","      4       0.7647        \u001b[32m0.7406\u001b[0m       \u001b[35m0.6197\u001b[0m        1.4251        81.3485\n","      5       0.8043        \u001b[32m0.6094\u001b[0m       \u001b[35m0.6471\u001b[0m        1.4408        82.7941\n","      6       0.8231        \u001b[32m0.5165\u001b[0m       0.6237        1.5391        82.0708\n","      7       0.8576        \u001b[32m0.4284\u001b[0m       0.6354        1.5353        81.6900\n","      8       0.8686        \u001b[32m0.3858\u001b[0m       \u001b[35m0.6481\u001b[0m        1.5754        81.5978\n","      9       0.9095        \u001b[32m0.2642\u001b[0m       0.6452        1.6386        80.8356\n","     10       0.9237        \u001b[32m0.2281\u001b[0m       0.6119        1.8822        81.0852\n","  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      dur\n","-------  -----------  ------------  -----------  ------------  ----  -------\n","      1       \u001b[36m0.0871\u001b[0m       \u001b[32m11.7727\u001b[0m       \u001b[35m0.3001\u001b[0m        \u001b[31m5.1187\u001b[0m     +  81.2657\n","      2       0.5176        \u001b[32m2.1611\u001b[0m       \u001b[35m0.5943\u001b[0m        \u001b[31m1.5899\u001b[0m     +  80.3626\n","      3       0.6852        \u001b[32m1.0924\u001b[0m       \u001b[35m0.6364\u001b[0m        \u001b[31m1.3697\u001b[0m     +  79.3827\n","      4       0.7485        \u001b[32m0.7871\u001b[0m       \u001b[35m0.6530\u001b[0m        \u001b[31m1.3150\u001b[0m     +  80.9639\n","      5       0.7696        \u001b[32m0.7369\u001b[0m       \u001b[35m0.6608\u001b[0m        \u001b[31m1.2391\u001b[0m     +  79.7294\n","      6       0.8280        \u001b[32m0.5391\u001b[0m       0.6510        1.4431        79.2744\n","      7       0.8523        \u001b[32m0.4584\u001b[0m       0.6276        1.5273        80.3814\n","      8       0.8772        \u001b[32m0.3724\u001b[0m       \u001b[35m0.6657\u001b[0m        1.5189        80.4994\n","      9       0.9092        \u001b[32m0.2753\u001b[0m       \u001b[35m0.6784\u001b[0m        1.3838        80.5053\n","     10       0.9244        \u001b[32m0.2252\u001b[0m       0.6588        1.4083        81.2035\n","  epoch    train_acc    train_loss    valid_acc    valid_loss    cp       dur\n","-------  -----------  ------------  -----------  ------------  ----  --------\n","      1       \u001b[36m0.2775\u001b[0m        \u001b[32m6.7656\u001b[0m       \u001b[35m0.5971\u001b[0m        \u001b[31m1.4264\u001b[0m     +  162.8385\n","      2       0.6793        \u001b[32m1.1163\u001b[0m       \u001b[35m0.6225\u001b[0m        \u001b[31m1.4263\u001b[0m     +  162.8566\n","      3       0.6950        \u001b[32m1.0929\u001b[0m       \u001b[35m0.6455\u001b[0m        1.4824        161.0263\n","      4       0.7469        \u001b[32m0.9284\u001b[0m       \u001b[35m0.6670\u001b[0m        \u001b[31m1.3831\u001b[0m     +  162.3300\n","      5       0.7753        \u001b[32m0.7779\u001b[0m       0.6587        1.4592        163.0793\n","      6       0.7915        \u001b[32m0.7096\u001b[0m       \u001b[35m0.6812\u001b[0m        1.4711        163.4090\n","      7       0.7866        0.7718       0.6318        1.9945        161.5964\n","      8       0.7815        0.9036       0.6029        2.4442        159.8244\n","      9       0.7740        1.0205       0.6210        2.5286        161.9548\n","     10       0.7738        1.1077       0.6367        2.9528        159.5876\n"]}]},{"cell_type":"code","source":["## Save search results\n","import pickle\n","\n","# Save the search object\n","with open(f'{gdrive_path}/grid_search_results.pkl', 'wb') as f:\n","    pickle.dump(search, f)"],"metadata":{"id":"SmXch6T0tRsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate matrix of predicted probabilities for the test set.\n","pred_prob = search.predict_proba(data_dict['test']['X'])"],"metadata":{"id":"uTDhXfT03lli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the pred_prob object\n","with open(f'{gdrive_path}/pred_prob.pkl', 'wb') as f:\n","    pickle.dump(pred_prob, f)"],"metadata":{"id":"8-L0U-H3fLIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pred_prob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ck1BhaUzknK8","executionInfo":{"status":"ok","timestamp":1712057396213,"user_tz":-480,"elapsed":510,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"1ffb3065-53b4-4f2c-c19e-baac4d17047a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.15323839e-09 1.58720177e-17 2.86248307e-19 ... 2.58924354e-21\n","  2.62892352e-22 1.32975296e-14]\n"," [8.39775791e-27 4.70906563e-33 6.33051386e-31 ... 2.59947617e-30\n","  8.35514677e-31 1.45292146e-30]\n"," [2.87608251e-17 1.22111292e-13 1.44774236e-16 ... 3.63143794e-14\n","  2.51296976e-13 2.49446341e-14]\n"," ...\n"," [2.87616840e-16 6.42553635e-17 1.65925506e-15 ... 9.06082143e-09\n","  3.51831786e-10 6.01781401e-14]\n"," [5.14962639e-12 5.35713546e-19 3.50179597e-22 ... 1.93557798e-17\n","  3.51228395e-15 2.21726131e-12]\n"," [2.10627439e-23 1.56444871e-10 3.91524105e-17 ... 7.89166562e-12\n","  1.10499815e-16 1.27379185e-24]]\n"]}]},{"cell_type":"markdown","source":["## Start Testing"],"metadata":{"id":"4gK01i4t4SVh"}},{"cell_type":"code","source":["# Generate csv file of predicted probabilities.\n","ids = sorted(os.listdir(os.path.join(f'{gdrive_path}', 'kaggle_dog/train_valid_test/test/unknown')))\n","\n","with open(f'{gdrive_path}/submission.csv', 'w') as f:\n","    f.write('id,' + ','.join(ds['train'].classes) + '\\n')\n","    for i, output in zip(ids, pred_prob):\n","        f.write(i.split('.')[0] + ',' + ','.join([str(num) for num in output]) + '\\n')"],"metadata":{"id":"Qquaa1Tw3tF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom callback to add training/validation loss/accuracy to Tensorboard.\n","class TensorboardMetrics(Callback):\n","    def __init__(self, tb):\n","        self.writer = tb\n","    # This runs after every epoch\n","    def on_epoch_end(self, net, **kwargs):\n","        self.train_loss = net.history[:, 'train_loss'][-1]\n","        self.valid_loss = net.history[:, 'valid_loss'][-1]\n","        self.train_acc = net.history[:, 'train_acc'][-1]\n","        self.valid_acc = net.history[:, 'valid_acc'][-1]\n","\n","        # Add current epoch training/validation loss to tensorboard.\n","        self.writer.add_scalars('Loss', {'Training Loss': self.train_loss, 'Validation Loss':\n","                                          self.valid_loss}, len(net.history))\n","\n","        # Add current epoch training/validation accuracy to tensorboard.\n","        self.writer.add_scalars('Accuracy', {'Training Accuracy': self.train_acc, 'Validation Accuracy':\n","                                            self.valid_acc}, len(net.history))"],"metadata":{"id":"hDnKZQoE4H9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve the best parameters found by GridSearchCV\n","best_params = search.best_params_\n","print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaVps8HIhWrF","executionInfo":{"status":"ok","timestamp":1712062242454,"user_tz":-480,"elapsed":335,"user":{"displayName":"Yeo Justin","userId":"13781415908193354515"}},"outputId":"7a3b3fe9-c7c2-4648-f44d-b2ed50f0076b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_size': 128, 'lr': 0.01}\n"]}]},{"cell_type":"code","source":["## Tensor board magic command\n","%reload_ext tensorboard\n","%tensorboard --logdir=f\"{gdrive_path}/test_run/dog-breed-identify\""],"metadata":{"id":"Zsx7mptIjqkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting up the writer for Tensorboard.\n","writer = SummaryWriter(f\"{gdrive_path}/test_run/dog-breed-identify\")\n","\n","class model(nn.Module):\n","    def __init__(self, pretrained_model):\n","        super().__init__()\n","        self.pretrained = pretrained_model\n","        # Add custom layers here.\n","        self.append1 = nn.Linear(250, 120)\n","\n","    def forward(self, X):\n","        # Compute the output given the input X\n","        X = self.pretrained(X)\n","        X = self.append1(X)\n","        return X\n","\n","# Defining the pretrained model.\n","pretrained_model = models.resnet18(weights='DEFAULT')\n","num_ftrs = pretrained_model.fc.in_features\n","for param in pretrained_model.parameters():\n","    param.requires_grad = False\n","pretrained_model.fc = nn.Linear(num_ftrs, 250)\n","\n","# Defining the model to be trained.\n","model = model(pretrained_model=pretrained_model)\n","model = model.to(device)\n","\n","# Creating checkpoints.\n","cp = Checkpoint(dirname=f\"{gdrive_path}/experiment13\", f_params=\"params_{last_epoch[epoch]}.pt\")\n","load_state = LoadInitState(cp)\n","\n","# Defining the neural net classifier.\n","net = NeuralNetClassifier(\n","    model,\n","    max_epochs=10,\n","    criterion=nn.CrossEntropyLoss(),\n","    lr=0.01,\n","    # Shuffle training data on each epoch\n","    iterator_train__shuffle=True,\n","    device = device,\n","    optimizer = torch.optim.Adam,\n","    batch_size = 128,\n","    callbacks=[cp, load_state, EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n","    TensorboardMetrics(tb=writer)],\n",")\n","\n","# Initializing the neural net classifier before loading parameters.\n","net.initialize()\n","net.load_params(f_params=f\"{gdrive_path}/experiment12/params_4.pt\")\n","net.history = net.history.from_file(f\"{gdrive_path}/experiment12/history.json\")\n","history = net.history\n","\n","# Adding the current history of metrics to Tensorboard.\n","for i in range(len(net.history)):\n","    if 'train_acc' in net.history[i]:\n","        writer.add_scalars('Loss', {'Training Loss': history[i]['train_loss'], 'Validation Loss':\n","                                    history[i]['valid_loss']}, i)\n","        writer.add_scalars('Accuracy', {'Training Accuracy': history[i]['train_acc'], 'Validation Accuracy':\n","                                          history[i]['valid_acc']}, i)\n","    else:\n","        writer.add_scalars('Loss', {'Training Loss': history[i]['train_loss'], 'Validation Loss':\n","                                    history[i]['valid_loss']}, i)\n","        writer.add_scalars('Accuracy', {'Validation Accuracy': history[i]['valid_acc']}, i)\n","\n","# Fitting the model to the training/validation set.\n","net.fit(data_dict['train_valid']['X'], data_dict['train_valid']['y'])\n","\n","\n","# Call flush() method to make sure that all pending events have been written to disk.\n","writer.flush()\n","\n","# If you do not need the summary writer anymore, call close() method.\n","writer.close()"],"metadata":{"id":"miI96ajF4IyG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"17f54879-0e1f-49fa-a051-e8efdbf4bd8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Re-initializing module.\n","Re-initializing criterion.\n","Re-initializing optimizer.\n","  epoch    train_acc    train_loss    valid_acc    valid_loss    cp       dur\n","-------  -----------  ------------  -----------  ------------  ----  --------\n","      2       \u001b[36m0.7335\u001b[0m        \u001b[32m1.2893\u001b[0m       \u001b[35m0.6205\u001b[0m        \u001b[31m2.5326\u001b[0m     +  171.7916\n","      3       \u001b[36m0.7231\u001b[0m        1.5534       \u001b[35m0.6401\u001b[0m        2.9467        170.7920\n","      4       0.7336        1.6933       0.6176        3.7767        176.9011\n","      5       0.7384        1.8476       0.5956        4.2639        176.4366\n","      6       0.7537        1.8554       0.6191        4.6584        177.7974\n","      7       0.7478        2.2641       0.5951        5.4450        175.5464\n","      8       0.7539        2.3766       \u001b[35m0.6411\u001b[0m        5.7421        175.6464\n","      9       0.7697        2.3375       0.6156        6.4036        176.3444\n","     10       0.7898        2.3670       0.6240        6.7343        176.0965\n"]}]},{"cell_type":"markdown","source":["some notes:\n","custom validation set and callbacks\n","grid search\n","resnet.history to get all the values of each epochs\n","PCA - closely linked to linear auotencoders"],"metadata":{"id":"z0eHylima45i"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}